{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tthustla/setiment_analysis_pyspark/blob/master/Sentiment%20Analysis%20with%20PySpark.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "import warnings\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import NGram, VectorAssembler\n",
    "from pyspark.ml.feature import ChiSqSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start a spark context\n",
    "sc=SparkContext.getOrCreate()   \n",
    "    \n",
    "spark = SparkSession.builder.appName(\"TweetSentiApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('datasetReviewed.csv')\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------+-------------------+--------------+-------------+-------------+--------------------+----------+\n",
      "|            tweet_id|        company|company_count|         created_at|favorite_count|retweet_count|  screen_name|               tweet|polaridade|\n",
      "+--------------------+---------------+-------------+-------------------+--------------+-------------+-------------+--------------------+----------+\n",
      "| 1128984581330407424|BANCO DO BRASIL|            1|2019-05-16 11:24:34|             0|            3|AgenciaEstado|RT @colunadobroad...|         0|\n",
      "| 1128600913092857856|       IGUATEMI|            1|2019-05-15 10:00:00|             4|            1|AgenciaEstado|Iguatemi vai repe...|      null|\n",
      "|https://t.co/ZTml...|              1|         null|               null|          null|         null|         null|                null|      null|\n",
      "| 1128245224294293504|            IRB|            1|2019-05-14 10:26:37|             0|            1|AgenciaEstado|RT @colunadobroad...|         1|\n",
      "| 1127542253159952384|           ITAÚ|            1|2019-05-12 11:53:16|             0|            4|AgenciaEstado|RT @colunadobroad...|         1|\n",
      "+--------------------+---------------+-------------+-------------------+--------------+-------------+-------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2667"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, val_set, test_set) = df.randomSplit([0.98, 0.01, 0.01], seed = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"polaridade\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------------+--------------+-------------+-------------+--------------------+----------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|   tweet_id|company|company_count|         created_at|favorite_count|retweet_count|  screen_name|               tweet|polaridade|               words|                  cv|            features|label|       rawPrediction|         probability|prediction|\n",
      "+-----------+-------+-------------+-------------------+--------------+-------------+-------------+--------------------+----------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|1,10735E+18|  CIELO|            1|2019-03-17 18:35:36|             5|            1|colunadobroad|Visa e Cielo lanç...|         0|[visa, e, cielo, ...|(7804,[0,5,6,11,2...|(7804,[0,5,6,11,2...|  0.0|[148.667419061282...|[1.0,2.1485922172...|       0.0|\n",
      "+-----------+-------+-------------+-------------------+--------------+-------------+-------------+--------------------+----------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineFit = pipeline.fit(train_set)\n",
    "train_df = pipelineFit.transform(train_set)\n",
    "val_df = pipelineFit.transform(val_set)\n",
    "train_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(train_df)\n",
    "predictions = lrModel.transform(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9393939393939393"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"polaridade\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineFit = pipeline.fit(train_set)\n",
    "predictions = pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8000\n",
      "ROC-AUC: 0.9899\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigrams(inputCol=[\"tweet\",\"polaridade\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"tweet\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=2**14,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"rawFeatures\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"polaridade\", outputCol = \"label\")]\n",
    "    selector = [ChiSqSelector(numTopFeatures=2**14,featuresCol='rawFeatures', outputCol=\"features\")]\n",
    "    lr = [LogisticRegression(maxIter=100)]\n",
    "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+selector+lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_pipelineFit = build_trigrams().fit(train_set)\n",
    "predictions = trigram_pipelineFit.transform(val_set)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n",
    "roc_auc = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8000\n",
      "ROC-AUC: 0.9798\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print (\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngrams_wocs(inputCol=[\"tweet\",\"polaridade\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"tweet\", outputCol=\"words\")]\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )]\n",
    "    label_stringIdx = [StringIndexer(inputCol = \"polaridade\", outputCol = \"label\")]\n",
    "    lr = [LogisticRegression(maxIter=100)]\n",
    "    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramwocs_pipelineFit = build_ngrams_wocs().fit(train_set)\n",
    "predictions_wocs = trigramwocs_pipelineFit.transform(val_set)\n",
    "accuracy_wocs = predictions_wocs.filter(predictions_wocs.label == predictions_wocs.prediction).count() / float(val_set.count())\n",
    "roc_auc_wocs = evaluator.evaluate(predictions_wocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8000\n",
      "ROC-AUC: 0.9798\n"
     ]
    }
   ],
   "source": [
    "# print accuracy, roc_auc\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy_wocs))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc_wocs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = trigramwocs_pipelineFit.transform(test_set)\n",
    "test_accuracy = test_predictions.filter(test_predictions.label == test_predictions.prediction).count() / float(test_set.count())\n",
    "test_roc_auc = evaluator.evaluate(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7500\n",
      "ROC-AUC: 0.8021\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: {0:.4f}\".format(test_accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
